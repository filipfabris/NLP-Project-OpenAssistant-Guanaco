{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\antoj\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e802870fb0a40108a03813011a7e7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(\"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\")\n",
    "\n",
    "zephyr_id = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    zephyr_id,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(zephyr_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_model(messages, model, tokenizer, terminators):\n",
    "\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages, add_generation_prompt=True, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=128,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    response = outputs[0][input_ids.shape[-1] :]\n",
    "    return tokenizer.decode(response, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
    "\n",
    "dataset_train = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "human_questions = []\n",
    "human_answer = []\n",
    "\n",
    "# for index in range(len(dataset_train[\"text\"])):\n",
    "for index in range(10):\n",
    "    string = dataset_train[\"text\"][index]\n",
    "    segments = string.split(\"### Human:\")\n",
    "    for segment in segments[1:]:\n",
    "        # This if is added to add to the list of questions only those that have then received an answer\n",
    "        if \"### Assistant:\" in segment:\n",
    "            human_questions.append(segment.split(\"### Assistant:\")[0].strip())\n",
    "\n",
    "    segments = string.split(\"### Assistant:\")\n",
    "    for segment in segments[1:]:\n",
    "        human_answer.append(segment.split(\"### Human:\")[0].strip())\n",
    "\n",
    "print(len(human_questions))\n",
    "print(len(human_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe with questions and answers\n",
    "import pandas as pd\n",
    "\n",
    "df_qa = pd.DataFrame(\n",
    "    list(zip(human_questions, human_answer)), columns=[\"question\", \"human_answer\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa = df_qa.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ —è–∑—ã–∫–µ swift, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–∞—Å—Å–∏–≤ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª, –∞ –∑–∞—Ç–µ–º –≤—ã–≤–æ–¥–∏—Ç –µ–≥–æ –Ω–∞ —ç–∫—Ä–∞–Ω',\n",
       " 'human_answer': '–í–æ—Ç —Ñ—É–Ω–∫—Ü–∏—è, –∫–æ—Ç–æ—Ä–∞—è —Å–æ—Ä—Ç–∏—Ä—É–µ—Ç –º–∞—Å—Å–∏–≤ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –∏ –≤—ã–≤–æ–¥–∏—Ç –µ–≥–æ –Ω–∞ —ç–∫—Ä–∞–Ω:\\n\\n```swift\\nfunc sortAndPrintArray(_ array: [Int]) {\\n  // –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –º–∞—Å—Å–∏–≤–∞, —á—Ç–æ–±—ã –Ω–µ –∏–∑–º–µ–Ω—è—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π\\n  var sortedArray = array\\n  // –°–æ—Ä—Ç–∏—Ä—É–µ–º –º–∞—Å—Å–∏–≤ –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é\\n  sortedArray.sort()\\n  // –í—ã–≤–æ–¥–∏–º –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–∞—Å—Å–∏–≤ –Ω–∞ —ç–∫—Ä–∞–Ω\\n  print(sortedArray)\\n}\\n```\\n\\n\\n–¢—ã –º–æ–∂–µ—à—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–∞–±–æ—Ç—É —Ñ—É–Ω–∫—Ü–∏–∏, –≤—ã–∑–≤–∞–≤ –µ–µ —Å –ª—é–±—ã–º –º–∞—Å—Å–∏–≤–æ–º —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª, –Ω–∞–ø—Ä–∏–º–µ—Ä:\\n\\n```swift\\nsortAndPrintArray([5, 2, 7, 9, -3])\\n```\\n\\n–ù–∞ —ç–∫—Ä–∞–Ω–µ –ø–æ—è–≤–∏—Ç—Å—è:\\n\\n```sh\\n[-3, 2, 5, 7, 9]\\n```\\n---\\n–ù–∞–¥–µ—é—Å—å, —á—Ç–æ —è —Å–º–æ–≥ —Ç–µ–±–µ –ø–æ–º–æ—á—å. –ù—É–∂–Ω–æ –ª–∏ —Ç–µ–±–µ —á—Ç–æ-–Ω–∏–±—É–¥—å –µ—â—ë? üòä'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary with questions and answers where index is the key\n",
    "qa_dict = df_qa.to_dict(orient=\"index\")\n",
    "qa_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_answers = []\n",
    "assistant_scores = []\n",
    "\n",
    "for index in range(5):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Normaly answer to the question\"\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": df_qa[\"question\"][index]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    response = chat_with_model(messages, model, tokenizer, terminators)\n",
    "    assistant_answers.append(response)\n",
    "    assistant_scores.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qa[\"assistant_answer\"] = assistant_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-Levenshtein\n",
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levenshtein Distance\n",
    "\n",
    "The Levenshtein distance between two strings $x = x_1 x_2 \\ldots x_m$ and $y = y_1 y_2 \\ldots y_n$ is defined as the minimum number of operations required to transform $x$ into $y$. The allowed operations are:\n",
    "\n",
    "1. Insertion of a character.\n",
    "2. Deletion of a character.\n",
    "3. Substitution of one character for another.\n",
    "\n",
    "#### Formal Definition\n",
    "\n",
    "Let $D(i, j)$ denote the Levenshtein distance between the prefixes $x_1 x_2 \\ldots x_i$ and $y_1 y_2 \\ldots y_j$. The matrix $D$ of dimensions $(m+1) \\times (n+1)$ is defined as:\n",
    "\n",
    "$$\n",
    "D(i, j) = \\begin{cases} \n",
    "i & \\text{if} \\; j = 0 \\\\\n",
    "j & \\text{if} \\; i = 0 \\\\\n",
    "\\min \\begin{cases} \n",
    "D(i-1, j) + 1 \\\\\n",
    "D(i, j-1) + 1 \\\\\n",
    "D(i-1, j-1) + \\delta(x_i, y_j) \n",
    "\\end{cases} & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\delta(x_i, y_j) = \\begin{cases} \n",
    "0 & \\text{if} \\; x_i = y_j \\\\\n",
    "1 & \\text{if} \\; x_i \\neq y_j \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[454, 1331, 1635, 982, 509]\n",
      "982.2\n"
     ]
    }
   ],
   "source": [
    "import Levenshtein\n",
    "\n",
    "predictions = list(df_qa[\"assistant_answer\"].astype(str))\n",
    "references = list(df_qa[\"human_answer\"].astype(str))\n",
    "\n",
    "distances = [Levenshtein.distance(t, p) for t, p in zip(references, predictions)]\n",
    "final_distance = sum(distances) / len(distances)\n",
    "\n",
    "print(final_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Error Rate (WER)\n",
    "\n",
    "The Word Error Rate (WER) is a metric used to evaluate the accuracy of an automatic speech recognition (ASR) system. It measures the number of errors in the transcribed output compared to a reference transcription, normalized by the total number of words in the reference.\n",
    "\n",
    "#### Definition\n",
    "\n",
    "WER is defined as:\n",
    "\n",
    "$$\n",
    "\\text{WER} = \\frac{S + D + I}{N}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $S$ is the number of substitutions.\n",
    "- $D$ is the number of deletions.\n",
    "- $I$ is the number of insertions.\n",
    "- $N$ is the total number of words in the reference transcription."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9200043682659775\n"
     ]
    }
   ],
   "source": [
    "import jiwer\n",
    "\n",
    "transformation = jiwer.Compose([\n",
    "    jiwer.ToLowerCase(),\n",
    "    jiwer.RemoveMultipleSpaces(),\n",
    "    jiwer.RemovePunctuation(),\n",
    "    jiwer.Strip(),\n",
    "    jiwer.ExpandCommonEnglishContractions()\n",
    "])\n",
    "\n",
    "errors = []\n",
    "for true, pred in zip(references, predictions):\n",
    "    transformed_true = transformation(true)\n",
    "    transformed_pred = transformation(pred)\n",
    "    wer_score = jiwer.wer(transformed_true, transformed_pred)\n",
    "    errors.append(wer_score)\n",
    "\n",
    "average_wer = sum(errors) / len(errors) if errors else 0\n",
    "\n",
    "print(average_wer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3678937538651776\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(references + predictions)\n",
    "\n",
    "similarities = [cosine_similarity(tfidf[i:i+1], tfidf[len(references)+i:len(predictions)+i+1])[0][0] for i in range(len(references))]\n",
    "similarity = sum(similarities) / len(similarities)\n",
    "\n",
    "print(similarity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
